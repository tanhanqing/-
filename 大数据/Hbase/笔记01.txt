zookeeper
---------------
	协同服务系统。
	分布式系统协同处理。
	小集群.

单机版
----------------
	文件系统层级结构存储。
	path:/home/centos/xxx = znode ---> byte[] <= 1M
	节点类型：1. 永久 2.临时 3.序列节点。10递增.
			  数据状态:版本.

	zkServer.sh start
	zkServer.sh status

	zkCli.sh -server s201:2181
	[zkCli]ls /
	[zkCli]create / ""
	[zkCli]get /
	[zkCli]set /
	[zkCli]rmr /

	api
	ZooKeeper zk = new ZooKeeper("s201:2181,s202:2181,...");



zk架构
------------------
	1.Client
		从server获取信息，周期性发送数据给server，表示自己还活着。
		client连接时，server回传ack信息。
		如果client没有收到reponse，自动重定向到另一个server.

	2.Server
		zk集群中的一员，向client提供所有service，回传ack信息给client，表示自己还活着。

	3.ensemble
		一组服务器。
		最小节点数是3.

	4.Leader
		如果连接的节点失败，自定恢复，zk服务启动时，完成leader选举。

	5.Follower
		追寻leader指令的节点。

znode
------------------
	zk中的节点，维护了stat，由Version number, Action control list (ACL), Timestamp,Data length.构成.
	data version		//数据写入的过程变化

	ACL					//action control list,


节点类型
-----------------
	1.持久节点
		client结束，还存在。
		
	2.临时节点
		在client活动时有效，断开自动删除。临时节点不能有子节点。
		leader推选是使用。

	3.序列节点
		在节点名之后附加10个数字，主要用于同步和锁.

Session
--------------------
	Session中的请求以FIFO执行，一旦client连接到server，session就建立了。sessionid分配client.

	client以固定间隔向server发送心跳，表示session是valid的，zk集群如果在超时时候，没有收到心跳，
	判定为client挂了，与此同时，临时节点被删除。

Watches
-------------------
	观察。
	client能够通过watch机制在数据发生变化时收到通知。
	client可以在read 节点时设置观察者。watch机制会发送通知给注册的客户端。
	观察模式只触发一次。
	session过期，watch机制删除了。


zk工作流程
----------------
	zk集群启动后，client连接到其中的一个节点，这个节点可以leader，也可以follower。
	连通后，node分配一个id给client，发送ack信息给client。
	如果客户端没有收到ack，连接到另一个节点。
	client周期性发送心跳信息给节点保证连接不会丢失。

	如果client读取数据，发送请求给node，node读取自己数据库，返回节点数据给client.


	如果client存储数据，将路径和数据发送给server，server转发给leader。
	leader再补发请求给所有follower。只有大多数(超过半数)节点成功响应，则
	写操作成功。

leader推选过程(最小号选举法)
-------------------
	1.所有节点在同一目录下创建临时序列节点。
	2.节点下会生成/xxx/xx000000001等节点。
	3.序号最小的节点就是leader，其余就是follower.
	4.每个节点观察小于自己节点的主机。(注册观察者)
	5.如果leader挂了，对应znode删除了。
	6.观察者收到通知。
	


配置完全分布式zk集群
---------------------
	1.挑选3台主机
		s201 ~ s203
	2.每台机器都安装zk
		tar
		环境变量

	3.配置zk配置文件
		s201 ~ s203
		[/soft/zk/conf/zoo.cfg]
		...
		dataDir=/home/centos/zookeeper

		server.1=s201:2888:3888
		server.2=s202:2888:3888
		server.3=s203:2888:3888

	4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3
		[s201]
		$>echo 1 > /home/centos/zookeeper/myid
		[s202]
		$>echo 2 > /home/centos/zookeeper/myid
		[s203]
		$>echo 3 > /home/centos/zookeeper/myid

	5.启动服务器集群 
		$>zkServer.sh start
		...

	6.查看每台服务器的状态
		$>zkServer.sh status

	7.修改zk的log目录
		


部署细节
----------------
	1.在jn节点分别启动jn进程
		$>hadoop-daemon.sh start journalnode

	2.启动jn之后，在两个NN之间进行disk元数据同步
		a)如果是全新集群，先format文件系统,只需要在一个nn上执行。
			[s201]
			$>hadoop namenode -format
		
		b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.
			1.步骤一
				[s201]
				$>scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/

			2.步骤二
				在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。
				[s206]
				$>hdfs namenode -bootstrapStandby		//需要s201为启动状态,提示是否格式化,选择N.
				
			3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。
				$>hdfs namenode -initializeSharedEdits
				#查看s202,s203是否有edit数据.

			4)启动所有节点.
				[s201]
				$>hadoop-daemon.sh start namenode		//启动名称节点
				$>hadoop-daemons.sh start datanode		//启动所有数据节点

				[s206]
				$>hadoop-daemon.sh start namenode		//启动名称节点
			

HA管理
-----------------
	$>hdfs haadmin -transitionToActive nn1				//切成激活态
	$>hdfs haadmin -transitionToStandby nn1				//切成待命态
	$>hdfs haadmin -transitionToActive --forceactive nn2//强行激活
	$>hdfs haadmin -failover nn1 nn2					//模拟容灾演示,从nn1切换到nn2

完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾
-------------------------------------------------
	1.停掉hadoop的所有进程
		
	2.删除所有节点的日志和本地数据.
	
	3.改换hadoop符号连接为ha

	4.登录每台JN节点主机，启动JN进程.
		[s202-s204]
		$>hadoop-daemon.sh start journalnode
	
	5.登录其中一个NN,格式化文件系统(s201)
		$>hadoop namenode -format

	6.复制201目录的下nn的元数据到s206
		$>scp -r ~/hadoop/* centos@s206:/home/centos/hadoop

	7.在未格式化的NN(s206)节点上做standby引导.
		7.1)需要保证201的NN启动
			$>hadoop-daemon.sh start namenode

		7.2)登录到s206节点，做standby引导.
			$>hdfs namenode -bootstrapStandby
		
		7.3)登录201，将s201的edit日志初始化到JN节点。
			$>hdfs namenode -initializeSharedEdits
		
	8.启动所有数据节点.
		$>hadoop-daemons.sh start datanode
	
	9.登录到206,启动NN
		$>hadoop-daemon.sh start namenode

	10.查看webui
		http://s201:50070/
		http://s206:50070/

	11.自动容灾
		11.1)介绍
			自动容灾引入两个组件，zk quarum + zk容灾控制器(ZKFC)。
			运行NN的主机还要运行ZKFC进程，主要负责:
			a.健康监控
			b.session管理
			c.选举
		11.2部署容灾
			a.停止所有进程
				$>stop-all.sh

			b.配置hdfs-site.xml，启用自动容灾.
				[hdfs-site.xml]
				<property>
					<name>dfs.ha.automatic-failover.enabled</name>
					<value>true</value>
				</property>

			c.配置core-site.xml，指定zk的连接地址.
				<property>
					<name>ha.zookeeper.quorum</name>
					<value>s201:2181,s202:2181,s203:2181</value>
				</property>

			d.分发以上两个文件到所有节点。
		
	12.登录其中的一台NN(s201),在ZK中初始化HA状态
		$>hdfs zkfc -formatZK
	
	13.启动hdfs进程.
		$>start-dfs.sh

	14.测试自动容在(206是活跃节点)
		$>kill -9

配置RM的HA自动容灾
----------------------
	1.配置yarn-site.xml
		<property>
			<name>yarn.resourcemanager.ha.enabled</name>
			<value>true</value>
		</property>
		<property>
			<name>yarn.resourcemanager.cluster-id</name>
			<value>cluster1</value>
		</property>
		<property>
			<name>yarn.resourcemanager.ha.rm-ids</name>
			<value>rm1,rm2</value>
		</property>
		<property>
			<name>yarn.resourcemanager.hostname.rm1</name>
			<value>s201</value>
		</property>
		<property>
			<name>yarn.resourcemanager.hostname.rm2</name>
			<value>s206</value>
		</property>
		<property>
			<name>yarn.resourcemanager.webapp.address.rm1</name>
			<value>s201:8088</value>
		</property>
		<property>
			<name>yarn.resourcemanager.webapp.address.rm2</name>
			<value>s206:8088</value>
		</property>
		<property>
			<name>yarn.resourcemanager.zk-address</name>
			<value>s201:2181,s202:2181,s203:2181</value>
		</property>
	
	2.使用管理命令
		//查看状态
		$>yarn rmadmin -getServiceState rm1
		//切换状态到standby
		$>yarn rmadmin -transitionToStandby rm1

	3.启动yarn集群
		$>start-yarn.sh

	4.hadoop没有启动两个resourcemanager,需要手动启动另外一个
		$>yarn-daemon.sh start resourcemanager

	5.查看webui

	6.做容灾模拟.
		kill -9


hive的注意事项
------------------
	如果配置hadoop HA之前，搭建了Hive的话，在HA之后，需要调整路径信息.
	主要是修改mysql中的dbs,tbls等相关表。

Hbase
--------------------
	hadoop数据库，分布式可伸缩大型数据存储。
	用户对随机、实时读写数据。
	十亿行 x 百万列。
	版本化、非关系型数据库。
	

Feature
----------------
	Linear and modular scalability.					//线性模块化扩展方式。
	Strictly consistent reads and writes.			//严格一致性读写
	Automatic and configurable sharding of tables	//自动可配置表切割
	Automatic failover support between RegionServers.	//区域服务器之间自动容在
	Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.		//
	Easy to use Java API for client access.			//java API
	Block cache and Bloom Filters for real-time queries	//块缓存和布隆过滤器用于实时查询 
	Query predicate push down via server side Filters	//通过服务器端过滤器实现查询预测
	Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options	//
	Extensible jruby-based (JIRB) shell					//
	Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX			//可视化
	面向列数据库。

hbase存储机制
-------------------
	面向列存储，table是按row排序。


搭建hbase集群
---------------
	0.选择安装的主机
		s201 ~ s204
	1.jdk
		略
	2.hadoop
		略
	3.tar 
		略
	4.环境变量
		略
	
	5.验证安装是否成功
		$>hbase version

	5.配置hbase模式
		5.1)本地模式
			[hbase/conf/hbase-env.sh]
			EXPORT JAVA_HOME=/soft/jdk

			[hbase/conf/hbase-site.xml]
			...
			<property>
				<name>hbase.rootdir</name>
				<value>file:/home/hadoop/HBase/HFiles</value>
			</property>

		5.2)伪分布式
			[hbase/conf/hbase-env.sh]
			EXPORT JAVA_HOME=/soft/jdk

			[hbase/conf/hbase-site.xml]
			<property>
				<name>hbase.cluster.distributed</name>
				<value>true</value>
			</property
			<property>
				<name>hbase.rootdir</name>
				<value>hdfs://localhost:8030/hbase</value>
			</property>

		5.3)完全分布式(必做)
			[hbase/conf/hbase-env.sh]
			export JAVA_HOME=/soft/jdk
			export HBASE_MANAGES_ZK=false

			[hbse-site.xml]
			<!-- 使用完全分布式 -->
			<property>
				<name>hbase.cluster.distributed</name>
				<value>true</value>
			</property>

			<!-- 指定hbase数据在hdfs上的存放路径 -->
			<property>
				<name>hbase.rootdir</name>
				<value>hdfs://s201:8020/hbase</value>
			</property>
			<!-- 配置zk地址 -->
			<property>
				<name>hbase.zookeeper.quorum</name>
				<value>s201:2181,s202:2181,s203:2181</value>
			</property>
			<!-- zk的本地目录 -->
			<property>
				<name>hbase.zookeeper.property.dataDir</name>
				<value>/home/centos/zookeeper</value>
			</property>
		
	6.配置regionservers
		[hbase/conf/regionservers]
		s202
		s203
		s204
	
	7.启动hbase集群(s201)
		$>start-hbase.sh
	
	8.登录hbase的webui
		http://s201:16010
		
