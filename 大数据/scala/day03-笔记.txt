scala
-------------
	class		//类
	object		//单例对象，静态成员所在组件。
	trait		//接口

	extends with xxx with yyy



模式匹配:类似于switch
---------------
	//1.
	val x = '9' ;
	x match{
		case '+' => print("+++")
		case '-' => print("+++")
		//携带守护条件
		case _ if Character.isDigit(x) => print("is number!");
		case _  => print("...");
	}

	//2.匹配类型,x类型定义成判断类型的共同超类。
	val x:Any = "123";
	x match{
		case b:Int => print("is Int") ;
		case a:String => print("is String") ;
		case _ => print("is Int") ;
	}

	//3.匹配数组
	val arr = Array(1,2)
	arr match{
		//匹配含有0
		case Array(0) => println("有0")
		//匹配是否两个元素
		case Array(x,y) => println("有两个元素")
		//是否从0开始
		case Array(0,_*) => println("从0开始")
		case _ => println("有0")
	}

变量声明模式
----------------
	val x = 100 ;			//
	val t = (1,2,3,4) ;		//元组
	val (a,b,c) = t			//解析元组中组员.

样例类
-----------------
	主要用于模式匹配.
	内置了apply和unapply方法，还有串行化等接口。
	创建对象时不需要使用new.

	abstract class Dog{}
	case class Jing8(name:String) extends Dog{}
	case class Shapi(age:Int) extends Dog{}

	val d:Dog = new Jing8("tom");
	d match{
		case Jing8(name) => print("是Jing8 : " + name);
		case Shapi(age) => print("是Shapi : " + age);
		case _ => print("aishiuihsui");
	}

密封样例类
---------------------
	子类和父类必须定义在同一文件中。
	sealed abstract class Dog{}
	case class Jing8(name:String) extends Dog{}
	case class Shapi(age:Int) extends Dog{}

偏函数
----------------------
	val f:PartialFunction[Char,Int] = {
		case '+' => 1 ; 
		case '-' => -1
		case _ => 0
	}

	val x = 'a'
	f(x)

泛型
---------------
	List<String>			//
	Map<String,String>		//
	
	//类的泛型,定义泛型类
	class Pair[T,S](one:T,second:S);		//定义泛型类
	val p = new Pair[String,Int]("tom",12);	//
	val p = new Pair("tom",12);				//类型推断
	

	//方法泛型
	def getMiddle[T](arr:Array[T]) = arr(arr.length / 2);

	//泛型的上界,T必须是Dog的子类。
	def run[T <: Dog](d:T) = println("hello")
	def run2[T >: Shapi](d:T) = println("hello")

	<:			//上界，子类
	>:			//下界，父类 ???
	<%			// A <% B,A能够隐式转换成B

	T <:Dog >:Cat		//约束多个条件。


型变
------------------
	Friend[+Dog]			//型变
	Friend[-Dog]			//逆变

	Friend[-Dog]

	Friend[Shapi]
	Friend[NafangShapi]


隐式转换
-------------------
	隐式转换函数:使用implicit修饰的具有一个参数的函数。

	//定义隐式转换函数
	implicit def int2Dog(n:Int) = Shapi(n)
	
	def run(d:Dog) = print("hello world");
	//调用隐式转换函数。
	run(100) ;

	//定义单例对象
	object DogUtil{
		//定义隐式转换函数
		implicit def str2Dog(s:String) = Jing8(s) ;
	}

	def run3(d:Dog) = println("hello world");

参数默认值
--------------------
	def decorate(prefix:String = "[[[",c:String,suffix:String="]]]") = ...
	decorate(c= "hello world")

隐式参数
----------------------
	
	object DogUtil2{
		implicit val dog = Jing8("tomas") ;
	}
	
	import DogUtil2._
	def run4(implicit dog:Jing8) = println("hello : ") ;

	run4();

并行
-------------
	集群计算。
	并行计算。

并发
-------------
	并发执行。



Spark
------------------------
	Lightning-fast cluster computing。
	快如闪电的集群计算。
	大规模快速通用的计算引擎。
	速度:	比hadoop 100x,磁盘计算快10x
	使用:	java / Scala /R /python
			提供80+算子(操作符)，容易构建并行应用。
	通用:	组合SQL ，流计算 + 复杂分析。

	运行：	Hadoop, Mesos, standalone, or in the cloud,local.
Spark模块
----------------
	Spark core		//核心模块
	Spark SQL		//SQL
	Spark Streaming	//流计算
	Spark MLlib		//机器学习
	Spark graph		//图计算



	DAG		//direct acycle graph,有向无环图。



安装Spark
-------------------
	1.下载spark-2.1.0-bin-hadoop2.7.tgz
		..
	2.解压
		..
	3.环境变量
		[/etc/profile]
		SPARK_HOME=/soft/spark
		PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
		
		[source]
		$>source /etc/profile

	4.验证spark
		
		$>cd /soft/spark
		$>./spark-shell

	5.webui
		http://s201:4040/

体验spark
-------------------

	0.sc
		SparkContext，Spark程序的入口点，封装了整个spark运行环境的信息。

	1.进入spark-shell
		$>spark-shell
		$scala>sc


API
--------------
	[SparkContext]
		Spark程序的入口点，封装了整个spark运行环境的信息。

	[RDD]
		resilient distributed dataset,弹性分布式数据集。等价于集合。




spark实现word count
------------------------
	//加载文本文件,以换行符方式切割文本.Array(hello  world2,hello world2 ,...)
	val rdd1 = sc.textFile("/home/centos/test.txt");

	//单词统计1
	$scala>val rdd1 = sc.textFile("/home/centos/test.txt")
	$scala>val rdd2 = rdd1.flatMap(line=>line.split(" "))
	$scala>val rdd3 = rdd2.map(word = > (word,1))
	$scala>val rdd4 = rdd3.reduceByKey(_ + _)
	$scala>rdd4.collect

	//单词统计2
	sc.textFile("/home/centos/test.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_ + _).collect

	//统计所有含有wor字样到单词个数。filter

	//过滤单词
	sc.textFile("/home/centos/test.txt").flatMap(_.split(" ")).filter(_.contains("wor")).map((_,1)).reduceByKey(_ + _).collect



[API]
	SparkContext:
		Spark功能的主要入口点。代表到Spark集群的连接，可以创建RDD、累加器和广播变量.
		每个JVM只能激活一个SparkContext对象，在创建sc之前需要stop掉active的sc。
	
	SparkConf:
		spark配置对象，设置Spark应用各种参数，kv形式。

	


编写scala程序，引入spark类库，完成wordcount
----------------------------------------------
	1.创建Scala模块,并添加pom.xml
		<?xml version="1.0" encoding="UTF-8"?>
		<project xmlns="http://maven.apache.org/POM/4.0.0"
				 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
				 xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
			<modelVersion>4.0.0</modelVersion>

			<groupId>com.it18zhang</groupId>
			<artifactId>SparkDemo1</artifactId>
			<version>1.0-SNAPSHOT</version>
			<dependencies>
				<dependency>
					<groupId>org.apache.spark</groupId>
					<artifactId>spark-core_2.11</artifactId>
					<version>2.1.0</version>
				</dependency>
			</dependencies>
		</project>
		
	2.编写scala文件
		import org.apache.spark.{SparkConf, SparkContext}

		/**
		  * Created by Administrator on 2017/4/20.
		  */
		object WordCountDemo {
			def main(args: Array[String]): Unit = {
				//创建Spark配置对象
				val conf = new SparkConf();
				conf.setAppName("WordCountSpark")
				//设置master属性
				conf.setMaster("local") ;

				//通过conf创建sc
				val sc = new SparkContext(conf);

				//加载文本文件
				val rdd1 = sc.textFile("d:/scala/test.txt");
				//压扁
				val rdd2 = rdd1.flatMap(line => line.split(" ")) ;
				//映射w => (w,1)
				val rdd3 = rdd2.map((_,1))
				val rdd4 = rdd3.reduceByKey(_ + _)
				val r = rdd4.collect()
				r.foreach(println)
			}
		}


java版单词统计
------------------
	import org.apache.spark.SparkConf;
	import org.apache.spark.SparkContext;
	import org.apache.spark.api.java.JavaPairRDD;
	import org.apache.spark.api.java.JavaRDD;
	import org.apache.spark.api.java.JavaSparkContext;
	import org.apache.spark.api.java.function.FlatMapFunction;
	import org.apache.spark.api.java.function.Function2;
	import org.apache.spark.api.java.function.PairFunction;
	import scala.Tuple2;

	import java.util.ArrayList;
	import java.util.Iterator;
	import java.util.List;

	/**
	 * java版
	 */
	public class WordCountJava2 {
		public static void main(String[] args) {
			//创建SparkConf对象
			SparkConf conf = new SparkConf();
			conf.setAppName("WordCountJava2");
			conf.setMaster("local");

			//创建java sc
			JavaSparkContext sc = new JavaSparkContext(conf);
			//加载文本文件
			JavaRDD<String> rdd1 = sc.textFile("d:/scala//test.txt");

			//压扁
			JavaRDD<String> rdd2 = rdd1.flatMap(new FlatMapFunction<String, String>() {
				public Iterator<String> call(String s) throws Exception {
					List<String> list = new ArrayList<String>();
					String[] arr = s.split(" ");
					for(String ss :arr){
						list.add(ss);
					}
					return list.iterator();
				}
			});

			//映射,word -> (word,1)
			JavaPairRDD<String,Integer> rdd3 = rdd2.mapToPair(new PairFunction<String, String, Integer>() {
				public Tuple2<String, Integer> call(String s) throws Exception {
					return new Tuple2<String, Integer>(s,1);
				}
			});

			//reduce化简
			JavaPairRDD<String,Integer> rdd4 = rdd3.reduceByKey(new Function2<Integer, Integer, Integer>() {
				public Integer call(Integer v1, Integer v2) throws Exception {
					return v1 + v2;
				}
			});

			//
			List<Tuple2<String,Integer>> list = rdd4.collect();
			for(Tuple2<String, Integer> t : list){
				System.out.println(t._1() + " : " + t._2());
			}
		}
	}


Spark2.1.0最新版是基于Scala2.11.8版本，因此安装scala2.11.8版本，
否则如果基于2.12.0版本编译会出现找不到包的问题。
----------------------------------------------
	1.卸载原来的scala.
	2.重新安装scala2.11.8版本
	3.配置idea的全局库
		project settings -> global library -> 删除原来的scala sdk
		project settings -> global library -> 添加sdk -> browser -> 定位scala安装目录 ->选中scala-compiler.jar + 
																						    scala-library.jar + 
																							scala-reflect.jar

	4.在模块中添加scala sdk 2.11.8版本
	
	5.重新编译项目 -> 导入jar ->丢到集群运行。

	
提交作业到spark集群运行
--------------------------
	1.导出jar包
	2.spark-submit提交命令运行job
		//Scala版本
		$>spark-submit --master local --name MyWordCount --class com.it18zhang.spark.scala.WordCountScala SparkDemo1-1.0-SNAPSHOT.jar /home/centos/test.txt
		//java版
		$>spark-submit --master local --name MyWordCount --class com.it18zhang.spark.java.WordCountJava SparkDemo1-1.0-SNAPSHOT.jar /home/centos/test.txt



Spark集群模式
-----------------
	1.local
		nothing!
		spark-shell --master local;		//默认

	2.standalone
		独立。
		a)复制spark目录到其他主机
		b)配置其他主机的所有环境变量
			[/etc/profile]
			SPARK_HOME
			PATH

		c)配置master节点的slaves
			[/soft/spark/conf/slaves]
			s202
			s203
			s204
		
		d)启动spark集群
			/soft/spark/sbin/start-all.sh

		e)查看进程
			$>xcall.jps jps
				master		//s201
				worker		//s202
				worker		//s203
				worker		//s204
		e)webui
			http://s201:8080/


提交作业jar到完全分布式spark集群
--------------------------------
	1.需要启动hadoop集群(只需要hdfs)
		$>start-dfs.sh
	2.put文件到hdfs.
		
	3.运行spark-submit
		$>spark-submit 
					--master spark://s201:7077 
					--name MyWordCount 
					--class com.it18zhang.spark.scala.WordCountScala 
					SparkDemo1-1.0-SNAPSHOT.jar 
					hdfs://s201:8020/user/centos/test.txt



脚本分析
-----------------------
	[start-all.sh]
		sbin/spark-config.sh
		sbin/spark-master.sh		//启动master进程
		sbin/spark-slaves.sh		//启动worker进程

	[start-master.sh]
		sbin/spark-config.sh
		org.apache.spark.deploy.master.Master
		spark-daemon.sh start org.apache.spark.deploy.master.Master --host --port --webui-port ...

	[spark-slaves.sh]
		sbin/spark-config.sh
		slaves.sh				//conf/slaves

	[slaves.sh]
		for conf/slaves{
			ssh host start-slave.sh ...
		}

	[start-slave.sh]
		CLASS="org.apache.spark.deploy.worker.Worker"
		sbin/spark-config.sh
		for ((  .. )) ; do
			start_instance $(( 1 + $i )) "$@"
		done 

	$>cd /soft/spark/sbin
	$>./stop-all.sh				//停掉整个spark集群.
	$>./start-master.sh			//停掉整个spark集群.
	$>./start-master.sh			//启动master节点
	$>./start-slaves.sh			//启动所有worker节点


